{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "003fd750",
   "metadata": {},
   "source": [
    "# Federated LLM Text Summarization (T5-small) — Notebook\n",
    "\n",
    "**Master’s Thesis (Mar–May 2024) — University of Skövde**  \n",
    "This notebook consolidates the project code (server, clients, and evaluation) for **federated fine-tuning** of **T5-small** using **Flower (FedAvg)**.  \n",
    "It is intended for local experimentation and reproducibility.\n",
    "\n",
    "> **Note:** The original repo organizes code under `client/`, `server/`, and `utils/`.  \n",
    "> For this notebook to run, make sure a `utils/` folder with `data_utils.py` and `model_utils.py` is available in the **same directory** as this notebook (or adjust imports accordingly).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07319b26",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Install dependencies (uncomment and run once per environment):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3674dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install torch transformers datasets sentencepiece rouge-score evaluate accelerate peft flwr pyyaml pydantic tqdm numpy pandas scikit-learn matplotlib flask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7f310bd",
   "metadata": {},
   "source": [
    "## 2. Dataset\n",
    "\n",
    "- Recommended dataset (example): **medical_meadow_cord19** from Hugging Face.  \n",
    "- Place your data per-client as needed, or adapt `utils/data_utils.py` to load it.\n",
    "\n",
    "Example link (from your README):  \n",
    "https://huggingface.co/datasets/medalpaca/medical_meadow_cord19\n",
    "\n",
    "> **Privacy:** Do not place sensitive/real patient data in this notebook or repo.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "694404e2",
   "metadata": {},
   "source": [
    "## 3. Server\n",
    "\n",
    "Runs the Flower server with a custom strategy that saves round models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352ea5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "from flwr.server.strategy import FedAvg\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from utils.model_utils import get_model, save_model\n",
    "import logging\n",
    "import torch\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class SaveModelStrategy(FedAvg):\n",
    "    def aggregate_fit(self, rnd, results, failures):\n",
    "        aggregated_weights = super().aggregate_fit(rnd, results, failures)\n",
    "        if aggregated_weights is not None:\n",
    "            model = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n",
    "            model.load_state_dict(dict(zip(model.state_dict().keys(), [torch.tensor(w) for w in aggregated_weights if w is not None])))\n",
    "            save_model(model, f\"model_round_{rnd}.pth\")\n",
    "            logging.info(f\"Model for round {rnd} saved.\")\n",
    "        return aggregated_weights\n",
    "\n",
    "\n",
    "def start_server():\n",
    "    logging.info(\"Loading pre-trained model...\")\n",
    "    model = get_model()\n",
    "    model_path = \"model_initial.pth\"\n",
    "    save_model(model, model_path)\n",
    "    logging.info(f\"Model loaded from {model_path}\")\n",
    "\n",
    "    strategy = SaveModelStrategy(\n",
    "        min_available_clients=2,\n",
    "        min_fit_clients=2,\n",
    "        min_eval_clients=2,\n",
    "        on_fit_config_fn=lambda rnd: {\"round\": rnd},\n",
    "    )\n",
    "\n",
    "    logging.info(\"Starting Flower server...\")\n",
    "    try:\n",
    "        fl.server.start_server(\n",
    "            server_address=\"0.0.0.0:8080\",\n",
    "            config={\"num_rounds\": 3},\n",
    "            strategy=strategy\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error starting server: {e}\", exc_info=True)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    start_server()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b8c897",
   "metadata": {},
   "source": [
    "## 4. Client\n",
    "\n",
    "Implements a Flower NumPyClient for local training/evaluation with ROUGE metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e62fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import flwr as fl\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import T5Tokenizer\n",
    "from utils.model_utils import get_model\n",
    "from utils.data_utils import preprocess_data, load_and_split_data\n",
    "from rouge_score import rouge_scorer\n",
    "import os\n",
    "import logging\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "class SummarizationClient(fl.client.NumPyClient):\n",
    "    def __init__(self, model, dataloader, tokenizer, client_id):\n",
    "        self.model = model\n",
    "        self.dataloader = dataloader\n",
    "        self.tokenizer = tokenizer\n",
    "        self.client_id = client_id\n",
    "        self.optimizer = torch.optim.Adam(model.parameters(), lr=5e-5)\n",
    "        self.scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        self.scores_file = Path(f\"client_{self.client_id}_scores.json\")\n",
    "\n",
    "    def get_parameters(self):\n",
    "        params = [val.cpu().numpy() for _, val in self.model.state_dict().items()]\n",
    "        logging.info(f\"[CLIENT {self.client_id}] get_parameters called\")\n",
    "        return params\n",
    "\n",
    "    def set_parameters(self, parameters):\n",
    "        params_dict = zip(self.model.state_dict().keys(), parameters)\n",
    "        state_dict = {k: torch.tensor(v, dtype=torch.float32) for k, v in params_dict}\n",
    "        self.model.load_state_dict(state_dict)\n",
    "        logging.info(f\"[CLIENT {self.client_id}] set_parameters called\")\n",
    "\n",
    "    def fit(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.train()\n",
    "        total_loss = 0\n",
    "        logging.info(f\"[CLIENT {self.client_id}] Training started\")\n",
    "        try:\n",
    "            for batch in self.dataloader:\n",
    "                self.optimizer.zero_grad()\n",
    "                logging.debug(f\"[CLIENT {self.client_id}] Processing batch: {batch}\")\n",
    "                inputs = preprocess_data(batch, self.tokenizer)\n",
    "                logging.debug(f\"[CLIENT {self.client_id}] Preprocessed inputs: {inputs}\")\n",
    "                outputs = self.model(**inputs)\n",
    "                loss = outputs.loss\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "                total_loss += loss.item()\n",
    "            logging.info(f\"[CLIENT {self.client_id}] Training completed, total loss: {total_loss}\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[CLIENT {self.client_id}] Error during training: {e}\", exc_info=True)\n",
    "\n",
    "        model_path = f\"model_client_{self.client_id}.pth\"\n",
    "        torch.save(self.model.state_dict(), model_path)\n",
    "        logging.info(f\"[CLIENT {self.client_id}] Model saved as {model_path}\")\n",
    "\n",
    "        return self.get_parameters(), len(self.dataloader.dataset), {}\n",
    "\n",
    "    def evaluate(self, parameters, config):\n",
    "        self.set_parameters(parameters)\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        rouge1, rouge2, rougel = 0, 0, 0\n",
    "        num_batches = 0\n",
    "        try:\n",
    "            with torch.no_grad():\n",
    "                logging.info(f\"[CLIENT {self.client_id}] Evaluation started\")\n",
    "                for batch in self.dataloader:\n",
    "                    inputs = preprocess_data(batch, self.tokenizer)\n",
    "                    outputs = self.model.generate(input_ids=inputs['input_ids'],\n",
    "                                                  attention_mask=inputs['attention_mask'], max_length=150)\n",
    "                    preds = self.tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "                    refs = self.tokenizer.batch_decode(inputs['labels'], skip_special_tokens=True)\n",
    "                    for pred, ref in zip(preds, refs):\n",
    "                        scores = self.scorer.score(ref, pred)\n",
    "                        rouge1 += scores['rouge1'].fmeasure\n",
    "                        rouge2 += scores['rouge2'].fmeasure\n",
    "                        rougel += scores['rougeL'].fmeasure\n",
    "                    num_batches += 1\n",
    "            avg_rouge1 = rouge1 / num_batches\n",
    "            avg_rouge2 = rouge2 / num_batches\n",
    "            avg_rougel = rougel / num_batches\n",
    "            logging.info(\n",
    "                f\"[CLIENT {self.client_id}] Evaluation completed, average loss: {total_loss / num_batches}, ROUGE-1: {avg_rouge1}, ROUGE-2: {avg_rouge2}, ROUGE-L: {avg_rougel}\")\n",
    "\n",
    "            round_scores = {\"round\": config[\"round\"], \"rouge1\": avg_rouge1, \"rouge2\": avg_rouge2, \"rougel\": avg_rougel}\n",
    "            if self.scores_file.exists():\n",
    "                with open(self.scores_file, \"r\") as f:\n",
    "                    all_scores = json.load(f)\n",
    "            else:\n",
    "                all_scores = []\n",
    "\n",
    "            all_scores.append(round_scores)\n",
    "\n",
    "            with open(self.scores_file, \"w\") as f:\n",
    "                json.dump(all_scores, f)\n",
    "\n",
    "            return total_loss / num_batches, len(self.dataloader.dataset), {\"rouge1\": avg_rouge1, \"rouge2\": avg_rouge2,\n",
    "                                                                            \"rougel\": avg_rougel}\n",
    "        except Exception as e:\n",
    "            logging.error(f\"[CLIENT {self.client_id}] Error during evaluation: {e}\", exc_info=True)\n",
    "            return total_loss / num_batches, len(self.dataloader.dataset), {\"rouge1\": 0, \"rouge2\": 0, \"rougel\": 0}\n",
    "\n",
    "\n",
    "def main(client_id):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "    model = get_model()\n",
    "\n",
    "    model_path = f\"model_client_{client_id}.pth\"\n",
    "    if os.path.exists(model_path):\n",
    "        logging.info(f\"[CLIENT {client_id}] Loading existing model for client {client_id}\")\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    else:\n",
    "        logging.info(f\"[CLIENT {client_id}] Training new model for client {client_id}\")\n",
    "\n",
    "    dataset = load_and_split_data(client_id)\n",
    "    dataloader = DataLoader(SummarizationDataset(dataset), batch_size=16, shuffle=True)\n",
    "\n",
    "    client = SummarizationClient(model, dataloader, tokenizer, client_id)\n",
    "\n",
    "    logging.info(f\"[CLIENT {client_id}] Client {client_id} connecting to server at localhost:8080\")\n",
    "    fl.client.start_numpy_client(\"localhost:8080\", client=client)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "\n",
    "    client_id = int(sys.argv[1])\n",
    "    main(client_id)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165be157",
   "metadata": {},
   "source": [
    "## 5. Evaluation of Aggregated Models\n",
    "\n",
    "Evaluates saved aggregated models across rounds and plots ROUGE scores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd19d3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import T5Tokenizer\n",
    "from rouge_score import rouge_scorer\n",
    "from utils.data_utils import load_local_data, preprocess_data\n",
    "from utils.model_utils import load_model\n",
    "import logging\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer):\n",
    "    model.eval()\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    rouge1, rouge2, rougel = 0, 0, 0\n",
    "    num_batches = 0\n",
    "    with torch.no_grad():\n",
    "        logging.info(\"Evaluation started\")\n",
    "        for batch in dataloader:\n",
    "            inputs = preprocess_data(batch, tokenizer)\n",
    "            outputs = model.generate(input_ids=inputs['input_ids'], attention_mask=inputs['attention_mask'])\n",
    "            preds = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            refs = tokenizer.batch_decode(inputs['labels'], skip_special_tokens=True)\n",
    "            for pred, ref in zip(preds, refs):\n",
    "                scores = scorer.score(ref, pred)\n",
    "                rouge1 += scores['rouge1'].fmeasure\n",
    "                rouge2 += scores['rouge2'].fmeasure\n",
    "                rougel += scores['rougeL'].fmeasure\n",
    "            num_batches += 1\n",
    "    avg_rouge1 = rouge1 / num_batches\n",
    "    avg_rouge2 = rouge2 / num_batches\n",
    "    avg_rougel = rougel / num_batches\n",
    "    logging.info(\n",
    "        f\"Evaluation completed, ROUGE-1: {avg_rouge1}, ROUGE-2: {avg_rouge2}, ROUGE-L: {avg_rougel}\")\n",
    "\n",
    "    return {\"rouge1\": avg_rouge1, \"rouge2\": avg_rouge2, \"rougel\": avg_rougel}\n",
    "\n",
    "\n",
    "def main():\n",
    "    model_name = 't5-small'\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "\n",
    "    test_data_file = \"dataset/medical_dataset.json\"\n",
    "    test_data = load_local_data(test_data_file)\n",
    "    test_dataloader = DataLoader(SummarizationDataset(test_data), batch_size=16, shuffle=False)\n",
    "\n",
    "    round_scores = []\n",
    "\n",
    "    for round_num in range(1, 4):  # Assuming 3 rounds, adjust if needed\n",
    "        model_path = f\"aggregated_model_round_{round_num}.pth\"\n",
    "        model = load_model(model_path, model_name)\n",
    "        scores = evaluate_model(model, test_dataloader, tokenizer)\n",
    "        scores[\"round\"] = round_num\n",
    "        round_scores.append(scores)\n",
    "\n",
    "    with open(\"aggregated_model_scores.json\", \"w\") as f:\n",
    "        json.dump(round_scores, f)\n",
    "\n",
    "    rounds = [score[\"round\"] for score in round_scores]\n",
    "    rouge1 = [score[\"rouge1\"] for score in round_scores]\n",
    "    rouge2 = [score[\"rouge2\"] for score in round_scores]\n",
    "    rougel = [score[\"rougel\"] for score in round_scores]\n",
    "\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(rounds, rouge1, label=\"ROUGE-1\", marker='o')\n",
    "    plt.plot(rounds, rouge2, label=\"ROUGE-2\", marker='o')\n",
    "    plt.plot(rounds, rougel, label=\"ROUGE-L\", marker='o')\n",
    "    plt.xlabel(\"Round\")\n",
    "    plt.ylabel(\"ROUGE Score\")\n",
    "    plt.title(\"ROUGE Scores of Aggregated Model\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"aggregated_model_rouge_scores.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb0a896",
   "metadata": {},
   "source": [
    "## 6. How to Run (Notebook workflow)\n",
    "\n",
    "Because federated training requires **concurrent** processes (server + multiple clients), the simplest flow is:\n",
    "\n",
    "1. **Start the server** by running the server cell (it will block).  \n",
    "   - Tip: Run it in one Jupyter **kernel** or start it externally with `python -m server.server`.\n",
    "2. **Start clients** in **separate** kernels/terminals by running the client cell and passing a `client_id`\n",
    "   (e.g., in a terminal: `python -m client.client 0`, `python -m client.client 1`, etc.).\n",
    "3. After training rounds complete, **run the evaluation** cell to compute ROUGE and plot results.\n",
    "\n",
    "If staying entirely in notebooks, you can also:\n",
    "\n",
    "- Convert the client code into a function `run_client(client_id)` and call it in separate notebook tabs or background sessions.\n",
    "- Or launch background processes using Python's `subprocess` from a cell (advanced).\n",
    "\n",
    "> **Web app (optional):** Your README mentions Flask apps under `web/` — you can add those cells similarly if needed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f11098d3",
   "metadata": {},
   "source": [
    "## 7. Original README (Reference)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692b62fe",
   "metadata": {},
   "source": [
    "```\n",
    "Federated Text Summarization using T5 Model\n",
    "\n",
    "This project explores the application of federated learning for fine-tuning a large language model (T5-small) for text summarization in the healthcare sector.\n",
    "The objective is to evaluate whether federated learning can enhance model performance while preserving data privacy across multiple decentralized clients.\n",
    "\n",
    "\n",
    "--Table of Contents--\n",
    ">Introduction\n",
    ">Project Structure\n",
    ">Setup and Installation\n",
    ">Dataset\n",
    ">Running the Project\n",
    ">Technologies Used\n",
    "\n",
    "\n",
    "\n",
    ">Introduction \n",
    "\n",
    "This project aims to fine-tune a T5 model for text summarization using federated learning, ensuring data privacy and security in the healthcare sector.\n",
    "The federated learning framework Flower is used to manage the communication and aggregation of model updates across multiple decentralized clients\n",
    "\n",
    "\n",
    "Project Structure\n",
    "federated_text_summarization_T5/\n",
    "├── client/\n",
    "│   ├── client.py\n",
    "├── server/\n",
    "│   ├── server.py\n",
    "├── utils/\n",
    "│   ├── data_utils.py\n",
    "│   ├── model_utils.py\n",
    "│   ├── __init__.py\n",
    "├── dataset/\n",
    "│   ├── medical_meadow_cord19.json\n",
    "├── web/\n",
    "│   ├── app.py\n",
    "│   ├── static/\n",
    "│   │   ├── script.js\n",
    "│   │   ├── style.css\n",
    "│   ├── templates/\n",
    "│   │   ├── index.html\n",
    "│──plot_decentralized_rouge_scores.py\n",
    "├──evaluate_aggregated_model.py\n",
    "├── README.md\n",
    "└── requirements.txt\n",
    "\n",
    "> Setup and Installation<\n",
    "\n",
    "pip install -r requirements.txt\n",
    "\n",
    ">Dataset\n",
    "> \n",
    "Download the dataset: https://huggingface.co/datasets/medalpaca/medical_meadow_cord19?row=13\n",
    "\n",
    "\n",
    ">Running the Project\n",
    "\n",
    "Run each command separate terminals\n",
    "\n",
    "\n",
    "Create a virtual environment:\n",
    "1.python -m venv env\n",
    "source env/bin/activate  # On Windows: `env\\Scripts\\activate`\n",
    "   \n",
    "\n",
    "Starting the server :\n",
    "1.python -m server.server\n",
    "\n",
    "\n",
    "Starting the Clients\n",
    "2.python -m client.client 0\n",
    "3.python -m client.client 1\n",
    "4.python -m client.client 2\n",
    "\n",
    "ROUGE Score Evaluation\n",
    "5.python -m evaluate_aggregated_model\n",
    "\n",
    "Starting the web Application\n",
    "6.python -m web.app\n",
    "7.python -m web.app\n",
    "8.python -m web.app\n",
    "\n",
    "\n",
    "\n",
    "Running the Web Application\n",
    "\n",
    "Access the web interface at http://127.0.0.1:5000.\n",
    "Access the web interface at http://127.0.0.1:5001.\n",
    "Access the web interface at http://127.0.0.1:5002.\n",
    "\n",
    "\n",
    ">Technologies Used\n",
    "\n",
    "PyTorch: For model training and optimization.\n",
    "Flower: Federated learning framework to manage client-server communication.\n",
    "Hugging Face Transformers: For using the T5 model.\n",
    "Flask: To deploy the web application.\n",
    "PyCharm IDE: For development and debugging.\n",
    "```"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
